{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkgTzA9DHM5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math, random, re, glob, codecs\n",
        "import urllib.request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxLqRFiFeRT",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "## Based on materials from Dr. Kevin Scannell\n",
        "\n",
        "In this lab, we will create our a Bayesian classifier.\n",
        "\n",
        "The problem we'll be looking at is one \"sentiment analysis\". For the purposes of this lab, we'll formulate the problem as a binary classifier in which we try to label texts as being either \"positive sentiment\" or \"negative sentiment\".  (Some formulations allow gradations, or just a third class for \"neutral sentiment\", etc., but I prefer having only two classes for this first example.)\n",
        "\n",
        "The training data we'll use consists of tweets written in the\n",
        "Irish language, gathered as part of one of Dr. Scannell's research projects. (Note from Dr. Scannell: \"I very intentionally chose a language that (I suspect) no one in the course actually speaks! This will prevent you from introducing any bias in your experiments, or injecting any intuition into how you model the problem.\")\n",
        "\n",
        "There are 10000 \"positive sentiment\" tweets in the file irish-happy.txt, and 10000 \"negative sentiment\" tweets in the file irish-sad.txt. Any personally identifying information has been stripped (usernames, etc.) Normally, assembling big sets of training data in machine learning is a huge challenge; very often the best way to achieve good results when building a classifier is to pay humans to label examples by hand to create a training set. In this case, Dr. Scannell took a big shortcut, and just gathered tweets containing either a happy or sad emoticon or emoji character!  So, strictly speaking, we're not really doing sentiment analysis, we're building a classifier that predicts whether a given tweet will contain a happy or frownie face, as a kind of \"proxy\" for sentiment analysis. As you'll see below, the results show this isn't completely\n",
        "unreasonable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdH1bHh9GnoA",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X2Y4a7KFPam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_labeled_data(url,label):\n",
        "    # load the content from the url\n",
        "    content = urllib.request.urlopen(url).read()\n",
        "\n",
        "    # decode it (make sure all the letters look right)\n",
        "    decoded = codecs.decode(content,encoding='utf-8')\n",
        "    \n",
        "    # split on the line breaks so that each tweet is an element of a list\n",
        "    split_list = decoded.split('\\n')\n",
        "\n",
        "    # create a list of tweets w/ the label that got passed in\n",
        "    tweets = []\n",
        "    for tweet in split_list:\n",
        "        tweets.append((tweet, label))\n",
        "\n",
        "    return tweets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfmjUT-gFQkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "irish_happy_url = 'https://raw.githubusercontent.com/kscanne/1070/master/lab04/irish-happy.txt'\n",
        "irish_sad_url = 'https://raw.githubusercontent.com/kscanne/1070/master/lab04/irish-sad.txt'\n",
        "\n",
        "# Load happy and sad tweets, labeling the happy ones as 'True' and the sad ones as 'False'\n",
        "happy_data = load_labeled_data(irish_happy_url,True)\n",
        "sad_data = load_labeled_data(irish_sad_url,False)\n",
        "\n",
        "# Concatenate the happy and sad data into one list\n",
        "full_dataset = happy_data.copy()\n",
        "full_dataset.extend(sad_data)\n",
        "\n",
        "# shuffle the dataset so we don't have all trues at the start and all falses at the end\n",
        "random.shuffle(full_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8-eBEg8QbyO",
        "colab_type": "text"
      },
      "source": [
        "Now we can take a look at the first five elements of our dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSrx7RUWOZvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "498a0956-5a79-4403-a8a7-29ea668ec845"
      },
      "source": [
        "full_dataset[:5]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('@USER t√°   is bre√° liom Barcelona is cathair iontach √©! T√° gr√° agam do Catal√≥in, √°it is fearr liom in Eoraip. T√°r √©is √âire cinnte üòâ',\n",
              "  False),\n",
              " ('@USER @USER @USER @USER Na b√≠ buartha. Beidh na pubanna foscailte  ', True),\n",
              " ('@USER An-s√°sta leis na p√≠obair√≠, leis na pi√≥ga steak & mushroom, agus leis an aimsir a bh√≠ muid, √°fach.   ',\n",
              "  True),\n",
              " (\"@USER  T√° Gaeilge ag go leor 'foreigners' anois  \", True),\n",
              " ('N√≠ bhfuair m√© ach ceithre huair I mo choladh ar√©ir, scriosta go hiomlan anois! Beidh s√© ag √©ir√≠ a bhfad n√≠os measa an bhliain seo #Strus  !',\n",
              "  False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVrezvaIQfhC",
        "colab_type": "text"
      },
      "source": [
        "# Training / Testing Split\n",
        "Now, let's split our data into a training and testing set, just like we did in our k-Nearest Neighbors classification. The code here is a bit different, but the output is the same (for example, if you pass in .75 as the prob parameter, then 75% of the data will get put into training, and 25% in testing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-nEFsOKRbT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(data, prob):\n",
        "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
        "    results = [], []\n",
        "    for row in data:\n",
        "        results[0 if random.random() < prob else 1].append(row)\n",
        "    return results\n",
        "\n",
        "train_data, test_data = split_data(full_dataset, 0.75)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnJOgHWmSK7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dbc27ae-20d2-4841-be07-c6d64cfda762"
      },
      "source": [
        "len(train_data), len(test_data)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13306, 4412)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-G7Uq9oTbLW",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Classifier\n",
        "\n",
        "The next big block of code actually implements the classifier. Later I'll have you come back and explore some different parts of this, but for now, just run this cell so that we can run our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hom3oAHQ7Wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(message):\n",
        "    message = message.lower()                       # convert to lowercase\n",
        "    patt = re.compile(u\"[a-z√°√©√≠√≥√∫'-]+\", re.UNICODE)\n",
        "    all_words = re.findall(patt, message)\n",
        "    return set(all_words)                           # remove duplicates\n",
        "\n",
        "def count_words(training_set):\n",
        "    \"\"\"training set consists of pairs (message, is_true)\"\"\"\n",
        "    counts = defaultdict(lambda: [0, 0])\n",
        "    for message, is_true in training_set:\n",
        "        for word in tokenize(message):\n",
        "            counts[word][0 if is_true else 1] += 1\n",
        "    return counts\n",
        "\n",
        "def word_probabilities(counts, total_true, total_false, k=0.5):\n",
        "    \"\"\"turn the word_counts into a list of triplets \n",
        "    w, p(w | true) and p(w | false)\"\"\"\n",
        "    return [(w,\n",
        "             (truec + k) / (total_true + 2 * k),\n",
        "             (falsec + k) / (total_false + 2 * k))\n",
        "             for w, (truec, falsec) in counts.items()]\n",
        "\n",
        "def true_probability(word_probs, message):\n",
        "    message_words = tokenize(message)\n",
        "    log_prob_if_true = log_prob_if_false = 0.0\n",
        "\n",
        "    for word, prob_if_true, prob_if_false in word_probs:\n",
        "        # for each word in the message, \n",
        "        # add the log probability of seeing it \n",
        "        if word in message_words:\n",
        "            log_prob_if_true += math.log(prob_if_true)\n",
        "            log_prob_if_false += math.log(prob_if_false)\n",
        "\n",
        "        # for each word that's not in the message\n",
        "        # add the log probability of _not_ seeing it\n",
        "        else:\n",
        "            log_prob_if_true += math.log(1.0 - prob_if_true)\n",
        "            log_prob_if_false += math.log(1.0 - prob_if_false)\n",
        "            \n",
        "    ans = 1.0 / (1.0 + math.exp(log_prob_if_false - log_prob_if_true))\n",
        "    return ans\n",
        "\n",
        "def p_true_given_word(word_prob):\n",
        "    word, prob_if_true, prob_if_false = word_prob\n",
        "    return prob_if_true / (prob_if_true + prob_if_false)\n",
        "  \n",
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, k=0.5):\n",
        "        self.k = k\n",
        "        self.word_probs = []\n",
        "\n",
        "    def train(self, training_set):\n",
        "        num_trues = len([is_true \n",
        "                         for message, is_true in training_set \n",
        "                         if is_true])\n",
        "        num_falses = len(training_set) - num_trues\n",
        "\n",
        "        # run training data through our \"pipeline\"\n",
        "        word_counts = count_words(training_set)\n",
        "        self.word_probs = word_probabilities(word_counts, \n",
        "                                             num_trues, \n",
        "                                             num_falses,\n",
        "                                             self.k)\n",
        "                                             \n",
        "    def classify(self, message):\n",
        "        return true_probability(self.word_probs, message)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH5rfLV6T8A-",
        "colab_type": "text"
      },
      "source": [
        "# Train!\n",
        "Now, we'll create an instance of the classifier class we defined above. This is like when we created an instance of the KNN classifier from sklearn, except this time, we implemented the classifier ourself rather than importing it from a library.\n",
        "\n",
        "Then we will train our classifier using our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDIecxb4RAW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = NaiveBayesClassifier()\n",
        "classifier.train(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4IK2_FURfT",
        "colab_type": "text"
      },
      "source": [
        "# Test!\n",
        "The following code will loop over the test dataset and predict whether each tweet is happy or sad (remember: true == happy; false == sad) using the classifier that was trained in the cell above.\n",
        "\n",
        "It will then print out the results of our classifier on\n",
        "our testing data, with an output that looks like this:\n",
        "\n",
        "Counter({(True, True): 1992, (False, False): 1389, (False, True): 579, (True, False): 471})\n",
        "\n",
        "This is saying that 1992 happy tweets we labeled as happy by\n",
        "the classifier, 1389 sad tweets were labeled as sad, 579 sad were labeled as happy, and 471 happy were labeled as sad.\n",
        "\n",
        "We can use this to compute the percentage labeled correctly, as a simple measure of the accuracy of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlQtOoyeRJvN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6041339d-9dcb-43cb-c5ab-ddf40ad68c23"
      },
      "source": [
        "classified = [(subject, is_true, classifier.classify(subject)) for subject, is_true in test_data]\n",
        "\n",
        "counts = Counter((is_true, true_probability > 0.5) # (actual, predicted)\n",
        "                     for _, is_true, true_probability in classified)\n",
        "\n",
        "print(counts)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 62\n",
            "Counter({(True, True): 1983, (False, False): 1396, (False, True): 524, (True, False): 509})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrB4KPV5Vdm2",
        "colab_type": "text"
      },
      "source": [
        "# HOMEWORK\n",
        "\n",
        "Please answer the following questions in a file type of your choosing (word document, Google Doc, slack post -- whatever!) and post it to your Slack channel before 11:59pm Sunday, April 5th.\n",
        "\n",
        "1) Reading the code. - 5pts\n",
        "\n",
        "(a) Recall that in our Naive Bayes model, we treat \"words\" as a features.  What is the place (the specific function or lines of code) where we break the texts into words (this relies on knowing any characters special to the language in question).\n",
        "\n",
        "(b) Can you find the place in the code where word probabilities are computed and \"smoothed\"? Hint: instead of \"adding 1\" to all word counts, this code adds a parameter \"k\" with default value 0.5...\n",
        "\n",
        "2) Compute the accuracy of our classifier. Remember that the training set is generated randomly each time we run the cell above that creates the training/testing split. Generate a handful of different splits, train the classifier on that new split, compute your accuracy, and average over all of the different datasets.\n",
        "\n",
        "3) Error analysis. The following code outputs the five \"truest false\" tweets, and the five \"falsest true\" tweets. The \"truest false\" tweets are the ones that are actually sad, but which \"look\" the happiest to the classifier. Similarly for \"falsest true\".\n",
        "\n",
        "For better or worse, Google Translate supports Irish-to-English translation; copy and paste these tweets into Google Translate and see if you can figure out why the classifier is particularly confused in these cases. Describe your explanation, using example tweets and translations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8CFemv4RL31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "7a5693b7-66c9-4faa-bc0f-06884a54d5ed"
      },
      "source": [
        "classified.sort(key=lambda row: row[2])\n",
        "truest_falses = list(filter(lambda row: not row[1], classified))[-5:]\n",
        "falsest_trues = list(filter(lambda row: row[1], classified))[:5]\n",
        "\n",
        "print(\"truest_falses\")\n",
        "for t in truest_falses:\n",
        "  print(t[0]+'\\n')\n",
        "\n",
        "print(\"\\n\\n\\nfalsest_trues\")\n",
        "for t in falsest_trues:\n",
        "  print(t[0]+'\\n')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truest_falses\n",
            "T√°imid ag √©isteacht le #bricfeastablasta deiridh @USER ar maidin ar @USER   Go n-√©ir√≠ an t-√°dh le do thogra√≠ nua go l√©ir, Lisa\n",
            "\n",
            "@USER n√≠l. ag fanacht ar uimhir nua a fh√°il... you can't get good help these days   \n",
            "\n",
            "@USER Aw n√° habair sin a Emma, n√≠ orainn an locht   S√∫il againn gur √©irigh go maith leat sa scr√∫d√∫ / Hope the exam went well! RRR.ie x\n",
            "\n",
            "@USER Loving the combined use of B√©arla agus Gaeilge ar an nuacht inniu! Pity it couldn't be like this gach l√°   #gaeilge #SnaG2016\n",
            "\n",
            "RT @USER: Campa Mhacha thart do bhliain eile   M√≠le bu√≠ochas leis na m√∫inteoir√≠ & c√∫nt√≥ir√≠. Go raibh maith agaibh speisialta leis na‚Ä¶\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "falsest_trues\n",
            "#Gaeilige @USER 'Northern Ireland....a nation State'!    Is ag magadh f√∫m at√° t√∫ nach ea? N√≠l ina 'nation State' ar chor ar bith √©.\n",
            "\n",
            "@USER t√° t√∫ ar meisce agus n√≠l s√© ach leath uair tar√©is a naoi   awh bhuel t√° m√© ag √≥l buid√©al Cobra m√© fh√©in\n",
            "\n",
            "Faraor g√©ar n√≠ bheidh an Club ar oscailt anocht d√° bharr fadhb aibhl√©ise. Gabhann muid √°r leithsc√©al, t√° s√∫l... {URL}\n",
            "\n",
            "#Gaeilige @USER diosp√≥ireacht iontach thar f√≥ir a ba √© faoin caighde√°n na Gaeilge nua-aimseartha anocht!!!    \n",
            "\n",
            "@USER Yeah, d'imigh s√© ar feadh 6/7 n√≥im√©ad.  Rinneas refreshing srl. ach n√≠ dhearna s√© aon rud de.  Th√°inig s√© ar ais ansin ina aonair.  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWxmUWKYW_LY",
        "colab_type": "text"
      },
      "source": [
        "5) The next cell shows the ten words most characteristic of happy tweets and sad tweets, respectively. Again, copy and paste these into Google Translate and see if they appear reasonable. Can you \"explain away\" any that don't appear reasonable?  Should we change the model in some way?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhWTWWpAW-fl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "0d1edabf-0494-4a38-ddaf-76573f1987d4"
      },
      "source": [
        "words = sorted(classifier.word_probs, key=p_true_given_word)\n",
        "\n",
        "truest_words = words[-10:]\n",
        "falsest_words = words[:10]\n",
        "\n",
        "print(\"truest_words:\")\n",
        "for t in truest_words:\n",
        "  print(t[0] + '\\n')\n",
        "\n",
        "print(\"\\n\\n\\nfalsest_words:\")\n",
        "for t in falsest_words:\n",
        "  print(t[0] + '\\n')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truest_words:\n",
            "brilliant\n",
            "\n",
            "happy\n",
            "\n",
            "shona\n",
            "\n",
            "gradaim\n",
            "\n",
            "seolta\n",
            "\n",
            "aithne\n",
            "\n",
            "greann\n",
            "\n",
            "breithe\n",
            "\n",
            "dhaoibh\n",
            "\n",
            "smile\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "falsest_words:\n",
            "ugh\n",
            "\n",
            "sl√°nlepeadar\n",
            "\n",
            "mins\n",
            "\n",
            "abalta\n",
            "\n",
            "cro√≠bhriste\n",
            "\n",
            "l√©arsc√°il\n",
            "\n",
            "arabacha\n",
            "\n",
            "mbreatain\n",
            "\n",
            "polaiti√∫il\n",
            "\n",
            "och√≥n\n",
            "\n",
            "ERROR! Session/line number was not unique in database. History logging moved to new session 64\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}